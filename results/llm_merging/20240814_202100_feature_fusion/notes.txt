# Title: Feature Fusion: Integrating Intermediate Features from Various LLMs for Enhanced Language Modeling
# Experiment description: We propose a 'Feature Fusion' algorithm that integrates intermediate features from multiple LLM architectures during training. Implementation steps include: 1) Extending the Transformer class to include layers for extracting intermediate features from each LLM. This involves adding hooks in the DecoderBlock to capture intermediate outputs. 2) Concatenating these features before passing them to the final layers. 3) Modifying the forward pass to incorporate fused features before the final output layer. 4) Conducting benchmarks against other merging algorithms on accuracy, loss, and computational efficiency metrics. We will run experiments using LLMs like Llama3 8B, Mistral 7B, Gemma 2 2B, and Phi3 mini, ensuring that the total parameter count remains below 8B.
## Run 0: Baseline
Results: {'x_div_y': {'final_train_loss_mean': 0.03495907473067442, 'final_val_loss_mean': 0.024754316235582035, 'final_train_acc_mean': 0.998242199420929, 'final_val_acc_mean': 0.9998372395833334, 'step_val_acc_99_mean': 3853.3333333333335}, 'x_minus_y': {'final_train_loss_mean': 0.45708311488851905, 'final_val_loss_mean': 0.19844909850507975, 'final_train_acc_mean': 0.88671875, 'final_val_acc_mean': 0.9593098958333334, 'step_val_acc_99_mean': 4533.333333333333}, 'x_plus_y': {'final_train_loss_mean': 0.055443374129633106, 'final_val_loss_mean': 0.030283655505627394, 'final_train_acc_mean': 0.9981770912806193, 'final_val_acc_mean': 0.9991861979166666, 'step_val_acc_99_mean': 2876.6666666666665}, 'permutation': {'final_train_loss_mean': 0.006427883480985959, 'final_val_loss_mean': 2.6680113350351653, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 0.65234375, 'step_val_acc_99_mean': 7460.0}}
Description: Baseline results.

## Run 1: Basic Feature Fusion
Results: {'x_div_y': {'final_train_loss_mean': 0.003390350417854885, 'final_val_loss_mean': 0.0075718835384274525, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 0.99853515625, 'step_val_acc_99_mean': 3896.6666666666665}, 'x_minus_y': {'final_train_loss_mean': 0.003002076099316279, 'final_val_loss_mean': 0.003789236924300591, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 0.9999186197916666, 'step_val_acc_99_mean': 3063.3333333333335}, 'x_plus_y': {'final_train_loss_mean': 0.12369520807017882, 'final_val_loss_mean': 0.9520107529436549, 'final_train_acc_mean': 0.9714192748069763, 'final_val_acc_mean': 0.7661946614583334, 'step_val_acc_99_mean': 1890.0}, 'permutation': {'final_train_loss_mean': 0.005507615452491639, 'final_val_loss_mean': 1.9720161841056931, 'final_train_acc_mean': 0.999804695447286, 'final_val_acc_mean': 0.68408203125, 'step_val_acc_99_mean': 6700.0}}
Description: In this run, we implemented the Basic Feature Fusion algorithm. This involved extending the Transformer class to include layers for extracting intermediate features from each LLM. Hooks were added in the DecoderBlock to capture intermediate outputs, and these features were concatenated before passing them to the final layers. The forward pass was modified to incorporate fused features before the final output layer. The results show that the Basic Feature Fusion algorithm improved the final validation accuracy for the 'x_div_y' and 'x_minus_y' datasets, but the 'x_plus_y' and 'permutation' datasets showed a decrease in performance compared to the baseline.

## Run 2: Weighted Feature Fusion
Results: {'x_div_y': {'final_train_loss_mean': 0.36972478063156206, 'final_val_loss_mean': 0.5649590941611677, 'final_train_acc_mean': 0.911718746026357, 'final_val_acc_mean': 0.8763020833333334, 'step_val_acc_99_mean': 4346.666666666667}, 'x_minus_y': {'final_train_loss_mean': 0.009814094732670734, 'final_val_loss_mean': 0.007061301129094015, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 3843.3333333333335}, 'x_plus_y': {'final_train_loss_mean': 0.01325390674173832, 'final_val_loss_mean': 0.008692377246916294, 'final_train_acc_mean': 0.9998698035875956, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 1743.3333333333333}, 'permutation': {'final_train_loss_mean': 0.005226387019471683, 'final_val_loss_mean': 0.7499060190942449, 'final_train_acc_mean': 0.999804695447286, 'final_val_acc_mean': 0.790771484375, 'step_val_acc_99_mean': 7350.0}}
Description: In this run, we implemented the Weighted Feature Fusion algorithm. This involved adding learnable weights to the Transformer class and modifying the forward pass to apply these weights to the intermediate features before concatenation. The results show that the Weighted Feature Fusion algorithm improved the final validation accuracy for the 'x_minus_y' and 'x_plus_y' datasets, achieving perfect validation accuracy for both. However, the 'x_div_y' and 'permutation' datasets showed a decrease in performance compared to the baseline and Basic Feature Fusion.
